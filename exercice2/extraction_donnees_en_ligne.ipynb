{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\udcca Extraction de Donn\u00e9es en Ligne\n", "Ce notebook contient les scripts d'extraction de donn\u00e9es depuis diff\u00e9rentes plateformes :\n", "- Amazon (BeautifulSoup)\n", "- Twitter (API v2)\n", "- Instagram (Graph API)\n", "- YouTube (Data API)\n", "- Google Search (`googlesearch`)\n", "- Reddit (PRAW)\n", "- Wikipedia (API Wikipedia)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# \ud83d\udce6 Installation des biblioth\u00e8ques (ex\u00e9cutez si n\u00e9cessaire)\n", "!pip install requests beautifulsoup4 tweepy google-api-python-client googlesearch-python praw wikipedia pandas"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 1. Amazon Web Scraping\n", "import requests\n", "from bs4 import BeautifulSoup\n", "import pandas as pd\n", "\n", "url = \"https://www.amazon.fr/s?k=ordinateur+portable\"\n", "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n", "\n", "response = requests.get(url, headers=headers)\n", "soup = BeautifulSoup(response.content, \"html.parser\")\n", "\n", "products = []\n", "for item in soup.select(\".s-main-slot .s-result-item\"):\n", "    title = item.select_one(\"h2 span\")\n", "    price = item.select_one(\".a-price .a-offscreen\")\n", "    if title and price:\n", "        products.append({\"Produit\": title.text, \"Prix\": price.text})\n", "\n", "df_amazon = pd.DataFrame(products)\n", "df_amazon.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 2. Twitter API v2\n", "import tweepy\n", "\n", "client = tweepy.Client(bearer_token=\"VOTRE_BEARER_TOKEN\")  # Remplace avec ta cl\u00e9\n", "query = \"Data Science\"\n", "tweets = client.search_recent_tweets(query=query, max_results=10, tweet_fields=[\"public_metrics\"])\n", "\n", "data = []\n", "for tweet in tweets.data:\n", "    metrics = tweet.public_metrics\n", "    data.append({\n", "        \"Tweet\": tweet.text,\n", "        \"Likes\": metrics[\"like_count\"],\n", "        \"Retweets\": metrics[\"retweet_count\"]\n", "    })\n", "df_twitter = pd.DataFrame(data)\n", "df_twitter.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 3. Instagram API (Graph)\n", "import requests\n", "\n", "access_token = \"VOTRE_TOKEN\"\n", "user_id = \"UTILISATEUR_ID\"\n", "url = f\"https://graph.instagram.com/{user_id}/media?fields=id,caption,media_url,like_count&access_token={access_token}\"\n", "response = requests.get(url)\n", "data = response.json()[\"data\"]\n", "\n", "posts = [{\"Caption\": post.get(\"caption\", \"\"), \"Likes\": post.get(\"like_count\", 0), \"Image\": post.get(\"media_url\", \"\")} for post in data]\n", "df_insta = pd.DataFrame(posts)\n", "df_insta.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 4. YouTube API\n", "from googleapiclient.discovery import build\n", "\n", "api_key = \"VOTRE_API_KEY\"\n", "youtube = build('youtube', 'v3', developerKey=api_key)\n", "\n", "request = youtube.search().list(q=\"Python programming\", part=\"snippet\", type=\"video\", maxResults=5)\n", "response = request.execute()\n", "\n", "videos = []\n", "for item in response[\"items\"]:\n", "    videos.append({\"Titre\": item[\"snippet\"][\"title\"], \"Cha\u00eene\": item[\"snippet\"][\"channelTitle\"]})\n", "df_youtube = pd.DataFrame(videos)\n", "df_youtube.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 5. Google Search\n", "from googlesearch import search\n", "\n", "query = \"Data Science cours gratuits\"\n", "results = list(search(query, num_results=10))\n", "df_google = pd.DataFrame(results, columns=[\"R\u00e9sultats\"])\n", "df_google.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 6. Reddit API (PRAW)\n", "import praw\n", "\n", "reddit = praw.Reddit(\n", "    client_id=\"VOTRE_ID\",\n", "    client_secret=\"VOTRE_SECRET\",\n", "    user_agent=\"testscript\"\n", ")\n", "\n", "posts = []\n", "for submission in reddit.subreddit(\"datascience\").hot(limit=5):\n", "    posts.append({\"Titre\": submission.title, \"Votes\": submission.score, \"Commentaires\": submission.num_comments})\n", "df_reddit = pd.DataFrame(posts)\n", "df_reddit.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# 7. Wikipedia API\n", "import wikipedia\n", "\n", "wikipedia.set_lang(\"fr\")\n", "content = wikipedia.page(\"Science des donn\u00e9es\").content\n", "df_wiki = pd.DataFrame([{\"Contenu\": content}])\n", "df_wiki.head(1)"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}